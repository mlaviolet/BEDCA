---
title: "Chapter 4, Part 1: Fitting Lines, Curves, and Surfaces"
# output: rmarkdown::html_vignette
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Chapter 4, Part 1: Fitting Lines, Curves, and Surfaces}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
  )
```

```{r setup, message=FALSE}
library(BEDCA)
library(mosaic)
library(tidyverse)
library(emmeans)
library(broom)
theme_set(theme_bw())
```

## 4.1: Fitting a Line by Least Squares

In Example 1 (p. 124) a group of students studied the effects of varying pressing pressures on the density of cylindrical specimens made by dry pressing a ceramic compound. Part of their data appears on Table 4.1 and data set `table4_01`. We've changed the units of pressure from psi to thousands of psi. First, make a scatterplot.

```{r}
ggformula::gf_point(density ~ pressure, data = table4_01,
                    xlab = "Pressure (psi x 1000)",
                    ylab = "Density (g/cc)") |> 
  ggformula::gf_lm(se = FALSE) # show the "best-fit" line
```

The `gf_lm()` operator adds the best fit line. Since that looks like a good fit, we'll proceed to fit the model using the `lm()` function ("lm" stands for "linear model"). The syntax is

$$\texttt{lm(y}\sim\texttt{1 + x)}$$

The response variable $y$ is `density`. We began our discussion of models with the null model. Now we'll fit a model adding an $x$ term for the single predictor `pressure`.

```{r}
lm_density <- lm(density ~ 1 + pressure, data = table4_01)
```

We can get minimal information by printing the model object.

```{r}
lm_density
```

The `Call:` section repeats the model we fit. The `Coefficients` section gives the computed slope and intercept. In this case, the fitted model is

$$\texttt{density = 2.37500 + 0.04867pressure}$$

We can get more information using the `msummary()` operator from the `mosaic` package. The base R operator `summary()` will produce a more extensive summary that we don't need yet.

```{r}
mosaic::msummary(lm_density)
```

Recall that the residuals are the differences between the data values and the model values; that is, the residuals represent the portion of the data not captured by the model. 

The model is fit applying the least squares principle: the coefficients are computed to minimize the sum of the squared residuals.squares. The column `Estimate` gives the coefficients. We'll discuss the details of computation shortly.

The slope is interpreted as the change in the response variable for an increase in the predictor variable of one unit. Here, for each increase of 1,000 (g/cc)/psi the density increases by 0.0487 g/cc. The intercept, the point at which the line crosses the *y*-axis, only makes practical sense if a pressure of 0 psi is meaningful. In almost all cases we focus attention on the slope.

As in Section 3.3, we can obtain fitted values and residuals with the `augment()` function from the `broom` package. The following produces the fitted values and residuals from the pressure-density data (Table 4.5, p. 136).

```{r}
lm_density_aug <- augment(lm_density)
lm_density_aug
```

`augment()` produces a data frame with the variables used to fit the model (`pressure`, `density`), the fitted values (`.fitted`), the residuals (`.resid`), and other information we don't need right now.

One purpose of fitting a model is to make predictions. If, for example, we want to predict the density of a cylinder pressed with 5,000 psi, we only need to substitute the desired pressure in the modeled linear equation.

$$\texttt{density = 2.37 + 0.0487} \times \texttt{5 = 2.619 g/cc}$$

To make predictions for other *x* values not in the original data, we construct a new data frame and apply the model.

```{r}
pressure_new <- data.frame(pressure = c(5, 7.5))
pressure_new
broom::augment(lm_density, newdata = pressure_new)
```

The `data.frame()` operator creates a data frame containing the desired predictor values; here, `pressure_new` is a data frame with one column `pressure` and two rows with the values 5 and 7.5. The column names must match those in the data used to fit the model.

Note that the new predictor values are within the range of the data. Take extreme caution if extrapolating outside that range.

In fitting any model, we need to check whether the model is a good fit to the data. Some subjectivity is involved, but we have tools from which we can make that determination. The most useful tool is to plot the residuals from the augmented data. First we'll plot the residuals against the fitted values.

```{r}
ggformula::gf_point(.resid ~ .fitted, data = lm_density_aug) |> 
  gf_hline(yintercept = 0, 
           linetype = "dashed",
           xlab = "Fitted density",
           ylab = "Residual")
```

If the model tells the full story about the data, then The residuals should be random noise. A plot of residuals against fitteds should show random scatter about zero. The `gf_hline()` operator adds a horizontal reference line as a visual aid.

Now we'll do a normal Q-Q plot of the residuals. Recall that a straight line plot indicates that the data follow a normal (bell-shaped) curve. This code produces Figure 4.7 (p. 136).

```{r}
ggformula::gf_qq(~ .resid, data = lm_density_aug,
                 ylab = "Residual quantile",
                 xlab = "Standard normal quantile") +
  coord_flip()
```

As the authors note, some idiosyncracies in the residual plots warrant further investigation. Normal Q-Q plots are discussed in more detail in Section 5.3.

Let's look at the rest of the brief summary.

```{r}
mosaic::msummary(lm_density)
```

The `Residual standard error` is the square root of the residual mean square. In the null model, we saw that the divisor for the residual mean square, called *degrees of freedom*, was $n-1$. For the simple linear regression model, the divisor is $n-2$. The general rule is

$$\texttt{Degrees of freedom = sample size - number of model terms}$$

For the density-pressure data we have $n$ = 15 and two model terms (slope and intercept), so degrees of freedom are 13 with a residual standard error of 0.01991.

We can also partition the variation. Recall from Section 3.3 that

$$\texttt{DATA}^2 = \texttt{FITTED}^2 + \texttt{RESIDUAL}^2$$

We can obtain the partition using the `anova()` function.

```{r}
anova(lm_density)
```

The `Df` column sums to 14, although the sample size is 15. This is because the null model is considered the baseline model and omitted from the report. The model degrees of freedom will always be the number of model terms less one.

The "fitted" (model) and "residual" components of variation are shown in the `Sum Sq` column. These add to a "total sum of squares," or `Sum Sq(total)` which R does not typically show. We can obtain the total sum of squares by multiplying the variance of the responses by the total degrees of freedom; that is, as $(n-1)s^2$. First we obtain the variance.

```{r}
var(~ density, data = table4_01)  
```

For the pressure-density data the sum of squares breakdown looks like this:

$$(15-1) \times 0.020669 = 0.289366 = 0.284213 + 0.005153$$

and the general rule is

$$\texttt{Sum Sq(total) = Sum Sq(model) + Sum Sq(Residuals)}$$

The last part of the report that we'll examine for now says `Multiple R-squared: 0.9822`. This is a measure of model fit formally called the *coefficient of determination*, but usually called "R-squared" for reasons we'll see shortly. R-squared is computed as

$$R^2=\frac{\texttt{Sum Sq(model)}}{\texttt{Sum Sq(model) + Sum Sq(Residuals)}}$$

The numerator is the variation due to the model, and the denominator is the total variation in the data. So R-squared is the proportion of variation in the data accounted for by the model. If the model fits the data perfectly, then it accounts for 100 percent of the variation and R-squared equals 1. If the model accounts for none of the variation in the data, then R-squared = 0 and the model is useless.

R-squared is straightforward to interpret: it always falls between 0 and 1 and has no units. For the pressure-density data

$$R^2=\frac{\texttt{0.284213}}{\texttt{0.284213 + 0.005153}} = \texttt{0.9822}$$

so the model accounts for 98.2 percent of the variation in cylinder density.

We'll look at the other parts of the regression report later. For now, in a simple linear regression

-   The `Estimate` column contains the coefficients computed according to the least-squares principle. The *y*-intercept is labelled `(Intercept)`, and the slope is labelled with the name of the predictor *x* variable.

-   `Residual standard error` (RSE) is the standard deviation of the residuals, computed with the divisor $n-2$. The smaller the RSE, the better the model fits the data.

-   `Multiple R-squared` gives the proportion of variation in the data that is accounted for by the model. R-squared is always between 0 and 1. All else being equal, a high R-squared indicates a good fit to the data.

R-squared gets its name because it's the square of the *correlation coefficient* $r$ between $x$ and $y$. The correlation coefficient measures the linear association between two quantitative variables and is computed with the R function `cor()`. For the pressure-density data

```{r}
mosaic::cor(pressure ~ density, data = table4_01)
```

and we have $0.991^2=0.982$.

The correlation coefficient $r$ has the following properties:

-   $-1 \le r\le 1$; $r$ equals $1 \: (-1)$ when all points fall on a line with positive (negative) slope

-   $r$ does not depend on which variable is the response and which is the predictor; that is, the correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$

-   $r$ has no units

-   $r$ measures linear association specifically, not association in general

As we saw in Section 3.3, fitting a model decomposes the data into a part explained by a a model and the part left over:

$$\texttt{DATA = FITTED + RESIDUAL}$$

Let's look again at the augmented data frame from the pressure-density data.

```{r}
head(lm_density_aug) # show first 6 rows
```

Plotting residuals is an impoortant tool for checking whether a model is appropriate for the data. This is done in several ways.

-   *Plots of residuals against fitted values.* Residuals should exhibit no patterns; that is, they should be randomly scattered about the zero horizontal line.

-   *Normal plots of residuals.* Points should fall along a straight line.

-   *Plots of residuals against all $x$ variables.* This applies to models having more than one predictor. All such plots should show random scatter.

-   *Plots of residuals against time order of observation.* These plots can reveal trends not seen in the original model.

-   *Plots of residuals against variables (like machine number or operator) not used in the fitted equation but potentially of importance.* By default, the augmented data frame contains only the variable used in the model fit. If that data frame contains additional variables, we can include them in the augmented data with by adding the argument `data = mydata` to `augment()`, where `mydata` is the complete data frame. These plots can reveal patterns that call for revising the model. 

## 4.2: Fitting Curves and Surfaces by Least Squares

### 4.2.1 Curve Fitting by Least Squares

Table 4.3, p. 133
Figure 4.9, p. 143

```{r}
ggformula::gf_point(strength ~ percent, data = table4_03) |> 
  gf_smooth(se = FALSE)
```

Correlation coefficient is nearly zero

```{r}
cor(strength ~ percent, data = table4_03)
```

Fitting quadratic to data in Table 4.3, p. 133 Actually multiple regression

```{r}
fly_ash_q <- lm(strength ~ 1 + percent + I(percent^2), data = table4_03)
mosaic::msummary(fly_ash_q)
```

Using orthogonal polynomials

```{r}
# x values are equally spaced, so okay
fly_ash_q2 <- lm(strength ~ 1 + poly(percent, 2), data = table4_03)
  mosaic::msummary(fly_ash_q2)
```

### 4.2.2 Surface Fitting by Least Squares

We'll begin this topic with Example 6, where a student studied the effects of positions, relative to the wing, of a canard (a forward lifting surface) and tail on the lift/drag ratio for a three-surface configuration.

```{r}
lm_add <- lm(lift_drag ~ 1 + canard + tail, data = table4_10)
mosaic::msummary(lm_add)
lm_add_aug <- augment(lm_add) |> 
  mutate(tail = factor(tail))
```

Graph for additive model

```{r}
ggformula::gf_point(.fitted ~ canard, data = lm_add_aug,
                    group = ~ tail,
                    shape = ~ tail) |>
  ggformula::gf_line(linetype = ~ tail,
                     show.legend = TRUE,
                     xlab = "Canard position",
                     ylab = "Fitted lift/drag ratio")  +
  scale_x_continuous(breaks = c(-1.2, 0, 1.2)) +
  theme(aspect.ratio = 0.62)
```

Full model with interaction term

```{r}
lm_full <- lm(lift_drag ~ 1 + canard + tail + canard:tail, data = table4_10)
mosaic::msummary(lm_full)
lm_full_aug <- augment(lm_full) |> 
  mutate(tail = factor(tail))
```

Fig 4.16, p. 157

```{r}
ggformula::gf_point(.fitted ~ canard, data = lm_full_aug,
                    group = ~ tail,
                    shape = ~ tail) |>
  ggformula::gf_line(linetype = ~ tail,
                     show.legend = TRUE,
                     xlab = "Canard position",
                     ylab = "Fitted lift/drag ratio")  +
  scale_x_continuous(breaks = c(-1.2, 0, 1.2)) +
  theme(aspect.ratio = 0.62)
```

```{r, eval=FALSE}
# Figure 4.16, ggplot version
ggplot(lm_full_aug) +
  aes(x = canard, y = .fitted, linetype = tail, group = tail, shape = tail) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = c(-1.2, 0, 1.2))
```

```{r, eval=FALSE}
# model equation 4.22, p. 158
lm_quad <- 
  lm(lift_drag ~ canard + tail + I(canard^2) + canard:tail, data = table4_10)

lm_quad2 <- 
  lm(lift_drag ~ poly(canard, 2) + tail + canard:tail, data = table4_10)
```

Subset of Brownlee stack loss data

```{r}
lm(stack ~ 1 + air + water + acid, data = table4_08)
```

Note shortcuts: intercept is not explicit; \* operator for interaction

<https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_interactions.html>

For exposition, we've included the intercept term in the model formula when using `lm()`. We can also fit a regression model with the syntax

$$\texttt{lm(y}\sim\texttt{x)}$$

The intercept term is included by default.

### Appendix

Consider quantitative variables $x$ and $y$. The *simple linear regression model* is

$$y \approx \beta_0 + \beta_1 x $$

that is, a straight line with hypothetical slope $\beta_1$ and hypothetical *y*-intercept $\beta_0$. "Simple" just means that the model has one quantitative predictor. The model fitted from data is written as

$$\hat{y} = b_0 + b_1 x $$

The Greek letters are replaced by Latin letters and the response variable has a "hat" to indicate that $y$ is estimated based on the computed $b_0$ and $b_1$.

The bivariate sample consists of $n$ ordered pairs $(x_1,y_1), (x_2,y_2), \ldots (x_n,y_n))$. The variables $x$ and $y$ have respective means $\bar{x}$ and $\bar{y}$, and standard deviations $s_x$ and $s_y$. The correlation coefficient $r$ is defined as

$$r = \frac{1}{n-1}\sum_{i=1}^{n}\left(\frac{x_i-\bar{x}}{s_x}\right)\left(\frac{y_i-\bar{y}}{s_y}\right)$$

The values of $x$ and $y$ are *standardized* to the number of standard deviations greater than or less than the mean. 

The least squares principle chooses the slope $b_1$ and intercept $b_0$ to minimize the sum of the squared residuals

$$\sum_{i=1}^{n}(y_i-\hat{y})^2$$
For the simple linear regression model predicting $y$ from $x$, the coefficients are

$$b_1 = r\frac{s_y}{s_x}$$

and

$$b_0 = \bar{y}-b_1 \bar{x}$$

The least squares line can thus be expressed in terms of the summaries of the individual variables $x$ and $y$ and the correlation $r$ between them.

The polynomial model is 

$$y \approx \beta_0 + \beta_1 x + \beta_2x^2 + \ldots + \beta_kx^k$$

and the general multiple regression model is

$$y \approx \beta_0 + \beta_1 x + \beta_2x_2 + \ldots + \beta_kx_k$$

For all the above models, the coefficients are computed by the least squares principle.
