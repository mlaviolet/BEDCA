---
title: "Chapter 4, Part 1: Fitting Lines, Curves, and Surfaces"
output: rmarkdown::html_vignette
# output: pdf_document
vignette: >
  %\VignetteIndexEntry{Chapter 4, Part 1: Fitting Lines, Curves, and Surfaces}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
  )
```

```{r setup, message=FALSE}
library(BEDCA)
library(mosaic)
library(tidyverse)
library(emmeans)
library(broom)
theme_set(theme_bw())
```

## 4.1: Fitting a Line by Least Squares

In Example 1 (p. 124) a group of students studied the effects of varying pressing pressures on the density of cylindrical specimens made by dry pressing a ceramic compound. Part of their data appears on Table 4.1 and data set `table4_01`. We've changed the units of pressure from psi to thousands of psi. First, make a scatterplot.

```{r}
ggformula::gf_point(density ~ pressure, data = table4_01,
                    xlab = "Pressure (psi x 1000)",
                    ylab = "Density (g/cc)") |> 
  ggformula::gf_lm(se = FALSE) # show the "best-fit" line
```

The `gf_lm()` operator adds the best fit line. Since that looks like a good fit, we'll proceed to fit the model using the `lm()` function ("lm" stands for "linear model"). The syntax is

$$\texttt{lm(y}\sim\texttt{1 + x)}$$

The response variable $y$ is `density`. We began our discussion of models with the null model. Now we'll fit a model adding an $x$ term for the single predictor `pressure`.

```{r}
lm_density <- lm(density ~ 1 + pressure, data = table4_01)
```

We can get minimal information by printing the model object.

```{r}
lm_density
```

The `Call:` section repeats the model we fit. The `Coefficients` section gives the computed slope and intercept. In this case, the fitted model is

$$\texttt{density = 2.37500 + 0.04867pressure}$$

We can get more information using the `msummary()` operator from the `mosaic` package. The base R operator `summary()` will produce a more extensive summary that we don't need yet.

```{r}
options('msummary.show.call' = TRUE) # add model call to output
mosaic::msummary(lm_density)
```

Recall that the residuals are the differences between the data values and the model values; that is, the residuals represent the portion of the data not captured by the model. 

The model is fit applying the least squares principle: the coefficients are computed to minimize the sum of the squared residuals.squares. The column `Estimate` gives the coefficients. We'll discuss the details of computation shortly.

The slope is interpreted as the change in the response variable for an increase in the predictor variable of one unit. Here, for each increase of 1,000 (g/cc)/psi the density increases by 0.0487 g/cc. The intercept, the point at which the line crosses the *y*-axis, only makes practical sense if a pressure of 0 psi is meaningful. In almost all cases we focus attention on the slope.

As in Section 3.3, we can obtain fitted values and residuals with the `augment()` function from the `broom` package. The following produces the fitted values and residuals from the pressure-density data (Table 4.5, p. 136).

```{r}
lm_density_aug <- augment(lm_density)
lm_density_aug
```

`augment()` produces a data frame with the variables used to fit the model (`pressure`, `density`), the fitted values (`.fitted`), the residuals (`.resid`), and other information we don't need right now.

One purpose of fitting a model is to make predictions. If, for example, we want to predict the density of a cylinder pressed with 5,000 psi, we only need to substitute the desired pressure in the modeled linear equation.

$$\texttt{density = 2.37 + 0.0487} \times \texttt{5 = 2.619 g/cc}$$

To make predictions for other *x* values not in the original data, we construct a new data frame and apply the model.

```{r}
pressure_new <- data.frame(pressure = c(5, 7.5))
pressure_new
broom::augment(lm_density, newdata = pressure_new)
```

The `data.frame()` operator creates a data frame containing the desired predictor values; here, `pressure_new` is a data frame with one column `pressure` and two rows with the values 5 and 7.5. The column names must match those in the data used to fit the model.

Note that the new predictor values are within the range of the data. Take extreme caution if extrapolating outside that range.

In fitting any model, we need to check whether the model is a good fit to the data. Some subjectivity is involved, but we have tools from which we can make that determination. The most useful tool is to plot the residuals from the augmented data. First we'll plot the residuals against the fitted values.

```{r}
ggformula::gf_point(.resid ~ .fitted, data = lm_density_aug) |> 
  gf_hline(yintercept = 0, 
           linetype = "dashed",
           xlab = "Fitted density",
           ylab = "Residual")
```

If the model tells the full story about the data, then The residuals should be random noise. A plot of residuals against fitteds should show random scatter about zero. The `gf_hline()` operator adds a horizontal reference line as a visual aid.

Now we'll do a normal Q-Q plot of the residuals. Recall that a straight line plot indicates that the data follow a normal (bell-shaped) curve. This code produces Figure 4.7 (p. 136).

```{r}
ggformula::gf_qq(~ .resid, data = lm_density_aug,
                 ylab = "Residual quantile",
                 xlab = "Standard normal quantile") +
  coord_flip()
```

As the authors note, some idiosyncracies in the residual plots warrant further investigation. Normal Q-Q plots are discussed in more detail in Section 5.3.

Let's look at the rest of the brief summary.

```{r}
mosaic::msummary(lm_density)
```

The `Residual standard error` is the square root of the residual mean square. In the null model, we saw that the divisor for the residual mean square, called *degrees of freedom*, was $n-1$. For the simple linear regression model, the divisor is $n-2$. The general rule is

$$\texttt{Degrees of freedom = sample size - number of model terms}$$

For the density-pressure data we have $n$ = 15 and two model terms (slope and intercept), so degrees of freedom are 13 with a residual standard error of 0.01991.

We can also partition the variation. Recall from Section 3.3 that

$$\texttt{DATA}^2 = \texttt{FITTED}^2 + \texttt{RESIDUAL}^2$$

We can obtain the partition using the `anova()` function.

```{r}
anova(lm_density)
```

The `Df` column sums to 14, although the sample size is 15. This is because the null model is considered the baseline model and omitted from the report. The model degrees of freedom will always be the number of model terms less one.

The "fitted" (model) and "residual" components of variation are shown in the `Sum Sq` column. These add to a "total sum of squares," or `Sum Sq(total)` which R does not typically show. We can obtain the total sum of squares by multiplying the variance of the responses by the total degrees of freedom; that is, as $(n-1)s^2$. First we obtain the variance.

```{r}
var(~ density, data = table4_01)  
```

For the pressure-density data the sum of squares breakdown looks like this:

$$(15-1) \times 0.020669 = 0.289366 = 0.284213 + 0.005153$$

and the general rule is

$$\texttt{Sum Sq(total) = Sum Sq(model) + Sum Sq(Residuals)}$$

The last part of the report that we'll examine for now says `Multiple R-squared: 0.9822`. This is a measure of model fit formally called the *coefficient of determination*, but usually called "R-squared" for reasons we'll see shortly. R-squared is computed as

$$R^2=\frac{\texttt{Sum Sq(model)}}{\texttt{Sum Sq(model) + Sum Sq(Residuals)}}$$

The numerator is the variation due to the model, and the denominator is the total variation in the data. So R-squared is the proportion of variation in the data accounted for by the model. If the model fits the data perfectly, then it accounts for 100 percent of the variation and R-squared equals 1. If the model accounts for none of the variation in the data, then R-squared = 0 and the model is useless.

R-squared is straightforward to interpret: it always falls between 0 and 1 and has no units. For the pressure-density data

$$R^2=\frac{\texttt{0.284213}}{\texttt{0.284213 + 0.005153}} = \texttt{0.9822}$$

so the model accounts for 98.2 percent of the variation in cylinder density.

We'll look at the other parts of the regression report later. For now, in a simple linear regression

-   The `Estimate` column contains the coefficients computed according to the least-squares principle. The *y*-intercept is labelled `(Intercept)`, and the slope is labelled with the name of the predictor *x* variable.

-   `Residual standard error` (RSE) is the standard deviation of the residuals, computed with the divisor $n-2$. The smaller the RSE, the better the model fits the data.

-   `Multiple R-squared` gives the proportion of variation in the data that is accounted for by the model. R-squared is always between 0 and 1. All else being equal, a high R-squared indicates a good fit to the data.

R-squared gets its name because it's the square of the *correlation coefficient* $r$ between $x$ and $y$. The correlation coefficient measures the linear association between two quantitative variables and is computed with the R function `cor()`. For the pressure-density data

```{r}
mosaic::cor(pressure ~ density, data = table4_01)
```

and we have $0.991^2=0.982$.

The correlation coefficient $r$ has the following properties:

-   $-1 \le r\le 1$; $r$ equals $1 \: (-1)$ when all points fall on a line with positive (negative) slope

-   $r$ does not depend on which variable is the response and which is the predictor; that is, the correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$

-   $r$ has no units

-   $r$ measures linear association specifically, not association in general

As we saw in Section 3.3, fitting a model decomposes the data into a part explained by a a model and the part left over:

$$\texttt{DATA = FITTED + RESIDUAL}$$

Let's look again at the augmented data frame from the pressure-density data.

```{r}
head(lm_density_aug) # show first 6 rows
```

Plotting residuals is an impoortant tool for checking whether a model is appropriate for the data. This is done in several ways.

-   *Plots of residuals against fitted values.* Residuals should exhibit no patterns; that is, they should be randomly scattered about the zero horizontal line.

-   *Normal plots of residuals*. Points should fall along a straight line.

-   *Plots of residuals against all $x$ variables*. This applies to models having more than one predictor. All such plots should show random scatter.

-   *Plots of residuals against time order of observation*. These plots can reveal trends not seen in the original model.

-   *Plots of residuals against variables (like machine number or operator) not used in the fitted equation but potentially of importance.* By default, the augmented data frame contains only the variable used in the model fit. If that data frame contains additional variables, we can include them in the augmented data with by adding the argument `data = mydata` to `augment()`, where `mydata` is the complete data frame. These plots can reveal patterns that call for revising the model. 

## 4.2: Fitting Curves and Surfaces by Least Squares

### 4.2.1 Curve Fitting by Least Squares

Fitting polynomials to data is an example of *multiple regression*, where the model contains more than one predictor. Example 2 (p. 132) introduces a study where a student examined the compressive strength of concrete-like fly ash cylinders. Data appear in Table 4.3 (p. 133) and in this package as `table4_03`. A scatterplot (Figure 4.9, p. 143) and analysis clearly show that a straight line is a poor fit. The function `gf_smooth()` below provides a general smooth.

```{r}
# Correlation coefficient is nearly zero
cor(strength ~ percent, data = table4_03)
```

```{r}
# Figure 4.9
ggformula::gf_point(strength ~ percent, data = table4_03) |> 
  gf_smooth(se = FALSE,
            xlab = "Percent ammonium phosphate",
            ylab = "Compressive strength (psi)")
```

To fit a quadratic curve, we need to add a squared term to the straight line model. This is accomplished with `I()`; this operator tells R that its argument is meant as an arithmetic operation instead of part of a model formula. 

```{r}
fly_ash_q <- lm(strength ~ 1 + percent + I(percent^2), data = table4_03)
mosaic::msummary(fly_ash_q)
```

The regression report shows that $R^2 = 0.867$. In multple regression, R-squared is the square of the coefficient between the observed and the fitted responses. The quadratic model accounts for 86.7 percent of the variation in compressive strength.

A more effective way to fit a polynomial is to use *orthogonal polynomial contrasts.* Orthogonal contrasts make the predictors uncorrelated, which is desirable when fitting a regression model. We use the `poly()` operator and let R handle the details. The second argument tells R the degree of polynomial to fit.

```{r}
fly_ash_q2 <- lm(strength ~ 1 + poly(percent, 2), data = table4_03)
  mosaic::msummary(fly_ash_q2)
```

Both methods of fitting a quadratic curve produce the same predicted values.

The authors also discuss fitting a cubic equation to the data. In fact, the smoothed scatterplot suggests that a cubic fit would be better than a quadratic. We need only change the second argument in `poly()` to 3.

```{r}
fly_ash_c <- lm(strength ~ 1 + percent + poly(percent, 3), data = table4_03)
mosaic::msummary(fly_ash_c)
```

The R-square increases to 0.952, but as the authors point out neither model adequately accounts for the three values where the ammonium phosphate level is 2%.

As with any model, residual plots would be run to assess the adequacy of the model fit. Also, note that with 18 observations and four model terms, the degrees of freedom are 14 (18 - 4).

### 4.2.2 Surface Fitting by Least Squares

We'll begin this topic with Example 6, where a student investigated the effects of positions, relative to the wing, of a canard (a forward lifting surface) and tail on the lift/drag ratio for a three-surface configuration.

Before we start fitting models, let's check the correlation between the two predictors.

```{r}
cor(canard ~ tail, data = table4_10)
```

The two predictors are uncorrelated, or orthogonal. Orthogonality is a desirable property because it allows us to analyze the impact of one factor without it being "confounded" or mixed up with the effects of another. We've already seen an example: in fitting orthogonal polynomials to data we can separate the effects of the linear, quadratic, and higher-order terms. 

Correlated predictors usually occur in observational studies and require some care in analysis and interpretation. Designed experiments, the focus of these notes, usually contain orthogonal predictors.

Now we'll fit a model to predict the lift/drag ratio using both `canard` and `tail` as predictors. 

```{r}
lm_add <- lm(lift_drag ~ 1 + canard + tail, data = table4_10)
mosaic::msummary(lm_add)
lm_add_aug <- augment(lm_add) |> 
  mutate(tail = factor(tail))
```

We'll call this model the *additive model*. For this model $R^2 = 0.394$
Next we'll construct an interaction plot using `tail` as the trace factor. Although `tail` is a quantitative variable, we need to make it a factor for the plot.

Note: when converting a quantitative variable to a factor, R recodes the factor levels to 1, 2, ... and the original values are discarded. This presents no problem for this example because the values of `tail` are evenly spaced.

```{r}
ggformula::gf_point(.fitted ~ canard, data = lm_add_aug,
                    group = ~ tail,
                    shape = ~ tail) |>
  ggformula::gf_line(linetype = ~ tail,
                     show.legend = TRUE,
                     xlab = "Canard position",
                     ylab = "Fitted lift/drag ratio")  +
  # enhancements
  scale_x_continuous(breaks = c(-1.2, 0, 1.2)) +
  theme(aspect.ratio = 0.62)
```

The plot shows three lines relating lift/drag to canard position, with one line for each tail position. The plot shows that tail position 1.2 produces a higher lift/drag ratio than tail position 0, which in turn produces a higher ratio than tail position -1.2. The parallel lines show that as the canard is raised, the change in ratio is the same for all three tail positions. Are the slopes possibly different? To investigate that, we need to add an interaction term to the model. In the R modeling language, the interaction term is specified as `canard:tail`.

```{r}
lm_full <- lm(lift_drag ~ 1 + canard + tail + canard:tail, data = table4_10)
mosaic::msummary(lm_full)
lm_full_aug <- augment(lm_full) |> 
  mutate(tail = factor(tail))
```

The $R^2$ has ingreased to $0.641$ but more importantly, raising the canard has different effects on the lift/drag ratio depending on the tail position. The inclusion of the interaction term allows the response curves to have different characteristics for different tail positions.

```{r}
ggformula::gf_point(.fitted ~ canard, data = lm_full_aug,
                    group = ~ tail,
                    shape = ~ tail) |>
  ggformula::gf_line(linetype = ~ tail,
                     show.legend = TRUE,
                     xlab = "Canard position",
                     ylab = "Fitted lift/drag ratio")  +
  scale_x_continuous(breaks = c(-1.2, 0, 1.2)) +
  theme(aspect.ratio = 0.62)
```

As the authors note, residual plotting shows that the model could be further refined.

```{r, eval=FALSE,echo=FALSE}
# Figure 4.16, ggplot version
ggplot(lm_full_aug) +
  aes(x = canard, y = .fitted, linetype = tail, group = tail, shape = tail) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = c(-1.2, 0, 1.2))
```

```{r, eval=FALSE,echo=FALSE}
# model equation 4.22, p. 158
lm_quad <- 
  lm(lift_drag ~ canard + tail + I(canard^2) + canard:tail, data = table4_10)

lm_quad2 <- 
  lm(lift_drag ~ poly(canard, 2) + tail + canard:tail, data = table4_10)
```

$$\texttt{RESUME HERE}$$

Subset of Brownlee stack loss data
Note correlation of variables--use cor() or some other function for correlation matrices

```{r}
lm(stack ~ 1 + air + water + acid, data = table4_08)
```

Note shortcuts: intercept is not explicit; \* operator for interaction

<https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_interactions.html>

For exposition, we've included the intercept term in the model formula when using `lm()`. We can also fit a regression model with the syntax

$$\texttt{lm(y}\sim\texttt{x)}$$

The intercept term is included by default.

### Appendix

Consider quantitative variables $x$ and $y$. The *simple linear regression model* is

$$y \approx \beta_0 + \beta_1 x $$

that is, a straight line with hypothetical slope $\beta_1$ and hypothetical *y*-intercept $\beta_0$. "Simple" just means that the model has one quantitative predictor. The model fitted from data is written as

$$\hat{y} = b_0 + b_1 x $$

The Greek letters are replaced by Latin letters and the response variable has a "hat" to indicate that $y$ is estimated based on the computed $b_0$ and $b_1$.

The bivariate sample consists of $n$ ordered pairs $(x_1,y_1), (x_2,y_2), \ldots (x_n,y_n))$. The variables $x$ and $y$ have respective means $\bar{x}$ and $\bar{y}$, and standard deviations $s_x$ and $s_y$. The correlation coefficient $r$ is defined as

$$r = \frac{1}{n-1}\sum_{i=1}^{n}\left(\frac{x_i-\bar{x}}{s_x}\right)\left(\frac{y_i-\bar{y}}{s_y}\right)$$

The values of $x$ and $y$ are *standardized* to the number of standard deviations greater than or less than the mean. 

The least squares principle chooses the slope $b_1$ and intercept $b_0$ to minimize the sum of the squared residuals

$$\sum_{i=1}^{n}(y_i-\hat{y})^2$$
For the simple linear regression model predicting $y$ from $x$, the coefficients are

$$b_1 = r\frac{s_y}{s_x}$$

and

$$b_0 = \bar{y}-b_1 \bar{x}$$

The least squares line can thus be expressed in terms of the summaries of the individual variables $x$ and $y$ and the correlation $r$ between them.

The polynomial model is 

$$y \approx \beta_0 + \beta_1 x + \beta_2x^2 + \ldots + \beta_kx^k$$

and the general multiple regression model is

$$y \approx \beta_0 + \beta_1 x + \beta_2x_2 + \ldots + \beta_kx_k$$

For all the above models, the coefficients are computed by the least squares principle.
