---
title: "Chapter 3: Elementary Descriptive Statistics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 3: Elementary Descriptive Statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
  )
```

```{r setup, message=FALSE, warning=FALSE}
library(BEDCA)
library(mosaic) 
library(dplyr)
library(tidyr)
library(broom)
theme_set(theme_bw())
```

## 3.1: Elementary Graphical and Tabular Treatment of Quantitatve Data

Chapter 3 discusses elementary graphical and numerical summaries: dot diagrams, histograms, run charts, quantiles, mean, standard deviation. Stem-and-leaf plots are omitted from this vignette. We also present a modeling approach to obtaining summary statistics.

### 3.1.1 Dot diagrams

Example 1 (p. 67) of this chapter revisits the heat-treating process for gears introduced in Chapter 1. As noted there, the data for the two gear treatments appear in separate columns, which is not amenable for analysis. Data must be reshaped into tidy format, with one column (`Treatment`) for the treatment type and a second (`Runout`) for the runout measurements.

```{r}
gears <- table1_01 |> 
    tidyr::pivot_longer(cols = everything(), 
                        values_to = "Runout", 
                        names_to = "Treatment") |> 
  # because the sample sizes are unequal one laid gear is shown as missing;
  #   remove it
  dplyr::filter(!is.na(Runout))
```

The best way to make a dot diagram is with the `dotPlot` function from the `mosaic` package. The syntax is

$$\texttt{dotPlot(}\sim \texttt{y | z, data = mydata)}$$

where `y` is the variable to be plotted and `z` is a panel variable; that is, separate plots are produced for each level of `z`.

This code replicates Figure 3.1 (p. 67). Note the `xlab` argument: always put meaningful labels on your graphs.

```{r}
mosaic::dotPlot(~ Runout | Treatment, data = gears,
                xlab = "Runout (0.0001 in.)")
```

Example 2 discusses a study to compare penetration depths of .45 caliber bullets fired into oak wood. Data appear in Table 3.1 (p. 67). Note that in `table3_01` the 230-grain bullets and the 200-grain bullets are in separate columns. For analysis and graphics we need to reshape the data into "tidy" format. The following code produces a new data set `bullets` with two columns: one (`grain`) for the type of bullet and a second (`depth`) for the depth measurements. This format allows us to group numeric and graphic output by bullet type. After reshaping, we can do a grouped dot diagram.

```{r}
bullets <- table3_01 |> 
      tidyr::pivot_longer(cols = everything(), 
                          names_to = "grain",
                          values_to = "depth")
mosaic::dotPlot(~ depth | grain, data = bullets,
                nint = 12,
                xlab = "Penetration depth (mm)")
```

### 3.1.2 Frequency Tables and Histograms

The following produces the data for Example 1, page 70. Since the measurements for laid gears and hung gears are in separate columns, we need only retain the laid gears with `dplyr::select()`. The `cut()` function divides the range of a quantitative variable into intervals and codes the values according to which interval they fall. The `breaks()` argument uses the `seq()` function to generate the sequence 5, 9, 13, ..., 32 and define the intervals [5,9), [9,13), ..., [25,28). The result of using `cut()`is a factor variable with the intervals as levels.

```{r}
gears_laid <- table1_01 |> 
  # "Laid" and "Hung" are in separate columns; keep "Laid"
  dplyr::select("Laid") |> 
  # drop the row with the "missing" runout
  tidyr::drop_na() |>  
  # divide data into intervals 5-8, 9-12, ..., 25-28
  dplyr::mutate(grp = cut(Laid, 
                          breaks = seq(from = 5, to = 32, by = 4), 
                          right = FALSE))
```

Now we compute the frequencies, relative frequencies, and cumulative relative frequencies from Table 3.2 on page 70:

```{r}
options(digits = 3)
gears_laid |> 
  dplyr::count(grp, .drop = FALSE) |>  # show intervals with zero count
  dplyr::mutate(rel_freq = n / sum(n), # divide count by number of runouts
         cum_freq = cumsum(rel_freq))  # cumulative relative frequency
```

Table 3.3 (p. 71) is constructed in a similar manner. Try it!

Figure 3.6 (p. 72) is a histogram of penetration depths for the 200-grain bullets. Since we already have the data reshaped, we'll do comparative histograms using both bullet types. The `gf_histogram()` function does the work of dividing the measurements into intervals. Usually the function makes good choices but we can refine the graph by specifying the number of intervals with the `bins` argument.

```{r}
ggformula::gf_histogram(~ depth | grain, data = bullets,
                        bins = 7, # use 7 intervals
                        xlab = "Penetration depth (mm)")
```

### 3.1.3 Scatterplots and Run Charts

Figure 3.9 (p. 75) shows a scatterplot of the torque required to loosen two bolts (designated Bolt 3 and Bolt 4) on the front plate of a heavy equipment component. In the figure presented in the textbook, the numbers in the plots indicate points falling on the same coordinates.

```{r}
gf_point(bolt4_torque ~ bolt3_torque, data = table3_04,
         xlab = "Bolt 3 torque (ft-lb)", 
         ylab = "Bolt 4 torque (ft-lb)")
```

To show distinct points we'll add a small amount of "jitter" to the scatterplot. Jittering adds a small amount of random variation to the point location so that all points are visible.

```{r}
gf_jitter(bolt4_torque ~ bolt3_torque, data = table3_04,
          # reduce jitter
          width = 0.2, height = 0.2, 
          xlab = "Bolt 3 torque (ft-lb)", 
          ylab = "Bolt 4 torque (ft-lb)")
```

A run chart is a scatterplot where the horizontal variable represents time. Run charts are usually drawn as points with connecting lines. In the following example (Figure 3.10, p. 76), the aspect ratio is changed to the "golden ratio" for aesthetics.

```{r}
gf_point(diameter ~ joint, data = table3_05) |> 
  gf_line() +
  theme(aspect.ratio = 0.62)
```

## 3.2 Quantiles and Related Graphical Tools

In this section, the authors consider numeric measures based on position and graphs based on them.

### 3.2.1 Quantiles and Quantile Plots

The text defines the $i$th smallest data point as the $\frac{i-0.5}{n}$ quantile. This is only one of many possible definitions; the authors chose it as the working definition. The $p$th quantile ($0\le{p}\le1$) is written as $Q(p)$.

The R function `quantile()` computes quantiles without the need for manual linear interpolation. The `probs` argument takes a vector containing the desired proportion(s). For example, to compute the 0.33 and 0.67 quantiles of the paper towel breaking strength data in Table 3.6, use this code:

```{r}
mosaic::quantile(~ strength, data = table3_06, probs = c(0.33, 0.67))
```

Note: The discreteness of finite data sets complicates the process of finding arbitrary quantiles. End users generally don't need to be concerned unless they need a definition for a specific purpose. Just for the record, R provides nine different algorithms for computing quantiles from discrete data. The R default is "Type 7" and the text's definition is "Type 5."

The R function to find the $i$th smallest data point is `rank()`. The smallest value is rank 1; the second-smallest rank 2; etc. The following code computes the data quantiles shown in Table 3.7 (p. 79).

```{r}
towels <- table3_06 |> 
  dplyr::mutate(i = rank(strength, ties.method = "random"),
                p = (i - 0.5) / length(i)) |> 
  dplyr::arrange(i)
```

A quantile plot is a plot of $Q(p)$ against $p$. The following replicates Figure 3.11, page 81:

```{r}
ggformula::gf_point(strength ~ p, data = towels) |> 
  gf_line() +
  theme(aspect.ratio = 0.62)
```

### 3.2.2 Boxplots

Most of this section uses the bullet penetration depth data from Table 3.1. The most straightforward way to compute summary statistics is with `mosaic::favstats()`.

```{r}
mosaic::favstats(depth ~ grain, data = bullets)
```

The first five numbers shown are known as the *five-number summary*: minimum, first quartile (Q1), median, third quartile (Q3), maximum. The range is the difference maximum - minimum. The interquartile range (IQR) is the difference Q3 - Q1; that is, the IQR is the range of the middle 50% of the data.

```{r}
mosaic::IQR(depth ~ grain, data = bullets)
```

This is the procedure for constructing a boxplot, also known as a box-and-whiskers plot.

-   Draw a box whose lower end is Q1 and whose upper end is Q3. The length of the box becomes the IQR.

-   The median is shown as a line inside the box.

-   The whiskers extend from the ends of the box to the minimum and maximum, unless any values are more than 1.5 box lengths from the end of the box. In that case, the whiskers extend only to the most extreme value within 1.5 box lengths and the values outside that range are plotted separately as outliers.

Boxplots are typically used for comparing distributions. Here's the code to produce Figure 3.14 on page 85.

```{r}
ggformula::gf_boxplot(depth ~ grain, data = bullets,
                      xlab = "Grain",
                      ylab = "Penetration depth (mm)")
```

Note that Figure 3.14 shows one outlier among the 200-grain bullets, while the R boxplot shows two. This difference is minor; due to roundoff in the text's manual calculations or differences in the algorithm for computing quantiles.

### 3.2.3 Q-Q Plots and Comparing Distributional Shapes

Normal plot, p. 90

This section discusses quantile plots as a means for comparing the shapes of distributions. In practce, quantiles of the data are plotted against the quantiles of a theoretical distribution. The theoretical distribution most used for this purpose is the *normal* distribution, which has the distinctive bell shape.

```{r, echo = FALSE}
df <- data.frame(x = c(-4, 4))
ggplot(data = df, aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  labs(x = "", y = "") +
  theme_bw() +
  theme(axis.ticks.x = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        aspect.ratio = 0.62)
```

The normal distribution is discussed in detail in Sections 5.2 and 5.3. For now, note the following:

-   The quantiles of the discrete data set are plotted against the quantiles of a hypothetical distribution.

-   If the plot tracks a straight line, then the two distributions have similar shapes.

The resulting plot is often called a *normal probability* plot or *normal plot*. To produce such a plot from the paper towel data, we need to add the normal distribution quantiles with the `qnorm()` function,

```{r}
towels <- towels |> 
  mutate(z = qnorm(p))
towels
```

Now we plot the normal distribution quantiles (called `z`) against the paper towel breaking strengths. This is Figure 3.18, p. 90.

```{r}
ggformula::gf_point(z ~ strength, data = towels,
                    xlab = "Breaking strength (g)",
                    ylab = "Standard normal quantile")
```

The plot is fairly linear; bear in mind that almost no real data set, especially if small, will produce a perfectly linear plot. We can use the `gf_qq()` function to directly produce a Q-Q plot.

```{r}
ggformula::gf_qq(~ strength, data = towels,
                 xlab = "Standard normal quantile",
                 ylab = "Breaking strength (g)") +
  # flip axes to replicate format in text
  coord_flip()
```

The `gf_qq()` function can also used to plot other distributions; the normal distribution is the default. These plots are discussed in detail in Section 5.3.

The default behavior also places the data on the vertical axis and the normal quantiles on the horizontal, which is the opposite of the text. The `coord_flip()` function will replicate the format in the text. Note that the labels are flipped along with the points.

## 3.3 Standard Numerical Summary Measures

The previous section presented measures of center and spread based on position: median, range, quartiles, interquartile range. This section discusses measures of center and spread based on sums: mean, variance, standard deviation.

### 3.3.1 Measures of location

The median and the mean (as statisticians like to call the average) are the two most common methods for identifying the "center" of a data set. The respective R functions are `median()` and `mean()`. The primary example in this section deals with percentage of waste on bulk paper rolls purchased from two different suppliers. To find the median by group with the formula interface, use `mosaic::median()`.

```{r}
mosaic::median(pct_waste ~ supplier, data = table3_13)
```

To produce a data frame we can use the tidyverse approach.

```{r}
table3_13 |> 
  dplyr::summarize(median_pct = median(pct_waste), .by = supplier)
```

We can find the mean percent waste by supplier in a similar manner.

```{r}
# formula interface
mosaic::mean(pct_waste ~ supplier, data = table3_13)
# tidyverse with dplyr()
table3_13 |> 
  dplyr::summarize(mean_pct = mean(pct_waste), .by = supplier)
```

### 3.3.2 Measures of spread

As seen in Section 3.2, the interquartile range (IQR) is the difference between the first and third quartiles; that is, the middle 50% of the data. The *range* is the difference between the maximum value and the minimum value. While the `IQR()` operator returns the IQR as a single number, the `range()` operator returns both the minimum and maximum values.

```{r}
mosaic::range(pct_waste ~ supplier, data = table3_13)
```

This output is a *named vector*, a vector whose components have names. In this case the names "11" and "12" refer to supplier 1 with the second digit showing "1" for the minimum and "2" for the maximum.

Methods of formal statistical inference are based on a measure of spread called the *standard deviation*, which measures spread about the mean. We'll develop the standard deviation using a modeling approach which will be followed throughout these notes. As our example we'll use the percent waste values for Supplier 1. To replicate a possible order of collection, we'll randomize the order of values.

```{r}
set.seed(42) # for reproducibility
supplier1 <- table3_13 |>
  filter(supplier == 1) |>  # note double equal sign for logical equality
  sample_frac(size = 1) |>  # size = 1 ensures all rows are included 
  select(pct_waste)         # only need values
supplier1
```

Chapter 2 introduced the taxonomy of variables. The variable of primary interest is called a *response variable*. The goal is to develop a model that predicts the response from other variables. Variables to which a researcher assigns values are called *experimental variables*, while variables not managed in the study are called *concomitant variables* or *covariates*. Experimental variables and covariates are collectively called *predictor variables* or *predictors*.

R uses formulas to define models. The general format for a model is `y ~ x`, where `y` is the response variable and `x` is one or more predictors. We'll start with the simplest possible model: one with no predictors. This model is called the \*null model\*. Developing the procedures for the null model will help us to understand more complex models.

Return to the waste data for Supplier 1 and suppose you had to predict the percent waste for the next shipment, so you have to choose a single number. How would you choose? Most people would use the average percent waste for a prediction. This turns out to be a sound choice, for reasons we'll develop shortly.

```{r}
supplier1
mosaic::mean(~ pct_waste, data = supplier1)
```

Looking at the data values and the mean, we can make two obvious but important observations: the values are not all the same, and none are equal to the mean. Statistics at its core is about two things:

-   Measuring variation

-   Distinguishing variation explained by the model from variation left unexplained

Another key observation is that some values are greater than the mean while others are less. The differences between each value and that predicted by the model are called the *residuals*. In other words, we can partiton each data value as

$$\texttt{DATA = FITTED + RESIDUAL}$$

Let's add fitted (predicted) values and residuals to our data.

```{r}
supplier1 <- supplier1 |> 
  mutate(.fitted = mean(pct_waste),
         .resid = pct_waste - .fitted)
supplier1
```

Small variation in the residuals indicates a well-fitting model. How do we measure variation? The residuals will always sum to zero, so that's no help. It turns out that the best way to eliminate the negative sign is to square the residuals.

```{r}
supplier1 <- supplier1 |> 
  mutate(resid_sq = .resid^2)
supplier1
```

Our intuition about the mean as an effective predictor is justified by the *principle of least squares*. Choosing the mean as a predictor in the null model will always minimize the sum of the squared residuals, or the *residual sum of squares*.

Our small sample has six values. What if we had 100? Clearly, a larger sample means more opportunity for a large residual sum of squares. We'd like to have a measure for a "typical" squared residual. Our first thought is to divide by $n$, but we need to adjust for the complexity of the model as well as the sample size. The more complex the model, the more data is "used up" in fitting it. Think of the data as currency; to fit a model you have to "spend" some of it. Spending data effectively reduces the sample size. The more complex the model, the more data you need to spend and the more the sample size decreases. We'll see how this works when we fit other models.

Our measure of a "typical" squared residual is called a *mean square*; for the null model the appropriate divisor is $n-1$ instead of $$n. Now we compute the mean square as the sum of the squared residuals divided by $n-1$.

```{r}
supplier1 |> 
  summarize(resid_mean_sq = sum(resid_sq) / (length(resid_sq) - 1))
```

For the null model the residual mean square has a special name: the *variance*. So in this case, our prediction is 1.495 percent and the variance is 1.945 ... what? The units of variance are the squared units of the data, which is not intuitive. To get back to the original units, take the square root.

```{r}
supplier1 |> 
  summarize(resid_mean_sq = sum(resid_sq) / (length(resid_sq) - 1)) |> 
  sqrt() |> 
  setNames("resid_std_err") # change name
```

The square root of the variance gives a measure of spread generically called the *residual standard error* and has the same units as the original data. For the null model, the residual standard error has a special name: the *standard deviation*. We can summarize our data as having a mean of 1.495 and a standard deviation of 1.394. See the results on pages 93 and 96 of the textbook.

Important note: Always report results to a sensible precision. Software can churn out as many decimal places as you want, but excess decimal places just look silly. A good practice is to report one decimal place beyond the original data.

Finally, we can partition the variation by computing the sum of the squared data values and the sum of the squared model values. We've already computed the residual sum of squares.

```{r}
supplier1 <- supplier1 |> 
  mutate(data_sq = pct_waste^2,
         model_sq = .fitted^2)
supplier1 |> 
  summarize(data_sum_sq = sum(data_sq),
            model_sum_sq = sum(model_sq),
            resid_sum_sq = sum(resid_sq))
```

Note that the model sum of squares (13.4) and the residual sum of squares (9.7) sum to the data sum of squares (23.1). This is not a coincidence. Like a right triangle, the following relationship holds for the models we will fit:

$$\texttt{DATA}^2 = \texttt{FITTED}^2 + \texttt{RESIDUAL}^2$$

We'll look at more streamlined ways to fit a model shortly. For now, let's summarize:

-   The mean is the least-squares measure of the center of a distribution; that is, the mean minimizes the sum of the squared residuals from the null model (containing no predictors).

-   The variance is the mean squared residual from the null model.

-   The standard deviation, the square root of the variance, is the residual standard error from the null model and has the same units as the data. The standard deviation measures spread about the mean.

-   The variation in the data, as measured by the sum of squares, can partitioned into a sum of squares due to the model and a sum of squares corresponding to variation left unexplained.

As noted in the text on page 98, at least 75% of a data set falls within two standard deviations of the mean; at least 89% falls within three standard deviations. The smaller the standard deviation, the more tightly the data are packed about their mean.

We can compute the mean and standard deviation directly with the following:

```{r}
mosaic::mean(~ pct_waste, data = supplier1)
mosaic::sd(~ pct_waste, data = supplier1)
```

Now we'll learn the specialized tools for fitting models. We'll begin with the null model and later apply these methods for more complex models.

The operator for fitting models is `lm()` for "linear model." The syntax is

$$\texttt{lm(y}\sim \texttt{x, data = mydata, ...)}$$

where `y` is a response variable, `x` is one or more predictor variables, 'mydata` is the data set containing `y` and `x`, and the triple dot `...` represents additional arguments as needed.

For the null model `x` is replaced by `1`. Let's fit the model using our example data.

```{r}
lm_null <- lm(pct_waste ~ 1, data = supplier1)
```

The result is assigned to an object `lm_null`, which contains all information about the fitted model. We'll use `summary()` to get some information. Note that the argument is the model object.

```{r}
summary(lm_null)
```

First, look at the `Coefficients:` table, `(Intercept)` row, `Estimate` column. The entry is `1.495`, which is the mean percentage waste. The line `Residual standard error: 1.39 on 5 degrees of freedom`, for this model, means that the standard deviation is 1.39 and that the divisor for finding the mean square is 5; that is, 6 - 1. That's all we need for now.

To see the variance, we call the `anova()` operator.

```{r}
anova(lm_null)
```

The number beneath `Mean Sq` is the variance; here 1.95.

Let's recap what we've done. In mathematical terms, the null model is represented as

$$y \approx \mu$$

read as "$y$ is approximately equal to $\mu$, where $\mu$ (Greek "m") is some constant. In the R language, the model is represented as

$$\texttt{lm(y}\sim \texttt{1)}$$

The model fitted from the data is

$$\hat{y}=1.495$$

where the "hat" above $y$ means "fitted" or "predicted."

Now we'll examine the fitted values and residuals, using the `augment()` operator from the `broom` package to create a data set.

```{r}
broom::augment(lm_null)
```

We only need the first three columns: the first contains the data values, the `.fitted` column contains the model fit, and `.resid` contains the residuals. In the null model, the values for `.fitted` are all the same. In Chapter 4, when we fit more interesting models, we'll see how to use this model data.

### 3.3.4 Plots of summary statistics

Plotting data values against time or the order of collection is a very useful technique. This code produces Figure 3.14 (p. 100). The graph shows the mean and range (maximum minus minimum) of 25 samples of five parts, each measuring a critical dimension of a metal part.

```{r}
# plot of means
ggformula::gf_point(xbar ~ sample, data = table3_14,
                    title = "Means chart",
                    xlab = "Sample",
                    ylab = "Mean") |> 
  gf_line() +
  theme(aspect.ratio = 0.62)
# plot of ranges
ggformula::gf_point(r ~ sample, data = table3_14,
                    title = "Range chart",
                    xlab = "Sample",
                    ylab = "Range") |>
  gf_line() +
  theme(aspect.ratio = 0.62)
```

As discussed in the text, the means for samples 9 through 15 are much less than the other samples. 

Example 9 (p. 101) discusses a student experiment studying the strength of glued wood joints with three woods and three glues. Figure 3.22 (p. 102) shows an *interaction plot*, which examines the pattern of differences between woods for each type of glue. In the graph, glues are the *x-factor* (the horizontal axis variable) and woods are the *trace factor* (the values connected with lines for each wood type).

The following code replicates Figure 3.22. In the `gf_line()` operator, `mean_strength` is the variable to plotted, `glue` is the x-factor, and `wood` is the trace factor.

```{r}
wood_joint <- table3_15 |> 
  # make wood and glue "factors" (categorical variables)
  # order levels to match Figure 3.22
  dplyr::mutate(wood = factor(wood, 
                              levels = c("pine", "fir", "oak")),
                glue = factor(glue, 
                              levels = c("white", "carpenter's", "cascamite")))

ggformula::gf_point(mean_strength ~ glue,
                    data = wood_joint,
                    group = ~ wood) |>
  gf_line(linetype = ~ wood, 
          show.legend = TRUE,
          xlab = "Glue type",
          ylab = "Strength (lb)") +
  theme(aspect.ratio = 0.35)
```

```{r echo=FALSE, eval=FALSE}
wood_joint <- table3_15 |> 
  # make wood and glue "factors" (categorical variables)
  # reorder levels to match order in textbook
  dplyr::mutate(wood = factor(wood, 
                              levels = c("pine", "fir", "oak")),
                glue = factor(glue, 
                              levels = c("white", "carpenter's", "cascamite")))

# fit model predicting strength from wood and glue combined
wood_joint_lm <- lm(mean_strength ~ wood * glue, data = wood_joint)

# make graph
emmeans::emmip(wood_joint_lm, 
               wood ~ glue,
               xlab = "Glue type",
               ylab = "Strength (lb)") +
  theme(aspect.ratio = 0.62)
```

The takeaways from this plot are

-   The differences in strength between pine and fir are roughly the same for all glue types.

-   The gluing properties of oak are very different from those of pine and fir.

This type of analysis is discussed in more detail in Section 4.4.

```{r echo=FALSE, eval=FALSE}
# black-and-white version
emmeans::emmip(wood_joint_lm, 
               wood ~ glue, 
               xlab = "Glue type",
               ylab = "Strength (lb)",
               # additional arguments for black-and-white
               col = "black",    
               linearg = list(), # pine = solid, fir = dashed, oak = longdash
               dotarg = list(size = 2)) + # pine = dot, fir = triangle, 
                                          # oak = square
  theme(aspect.ratio = 0.62)
```

### 3.4 Descriptive Statistics for Qualitative [Categorical] and Count Data

UPDATE FUNCTION LIST
TWEAK PLOTS
ADD LABELS

Figure 3.23, p. 108

Note difference between gf_col and gf_bar

```{r}
ggformula::gf_col(connectors ~ category, data = table3_16,
                  fill = "steelblue")
```

Figure 3.24, p. 108

```{r}
ggformula::gf_col(n_tools ~ problem, data = table3_17,
                  fill = "steelblue")
```

Figure 3.25, p. 110

```{r}
ggformula::gf_col(pct ~ job, data = table3_18,
                  fill = ~ type)
```

Figure 3.26, p. 111

```{r}
ggformula::gf_point(defects_truck ~ day,
                    data = table3_19) |>
  gf_line()
```

Figure 3.27, p. 112

```{r}
ggformula::gf_point(n_conform ~ shot_size,
                    group = ~ mixture,
                    data = table3_20) |>
  gf_line(linetype = ~ mixture, 
          show.legend = TRUE)
```


